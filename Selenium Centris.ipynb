{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Scrape the centris gallery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# the lists that will hold the scraped data - Run this cell to reset the lists!\n",
    "\n",
    "prices = []\n",
    "ppsf = []\n",
    "ppsm = []\n",
    "categories = []\n",
    "streets = []\n",
    "cities = []\n",
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     19,
     42,
     71,
     78,
     84,
     105,
     110,
     120,
     153,
     166
    ],
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#imports and function definitions\n",
    "%config Completer.use_jedi = False\n",
    "%config IPCompleter.greedy=True\n",
    "import time\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "my_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#driver.quit() terminates the driver object whereas driver.close() just closes the window... use the former!\n",
    "\n",
    "#proxy tester\n",
    "#scrapes the \"http://proxydb.net/anon\" page and displays results\n",
    "def proxy_anonymity_test_results(proxydb_html_page):\n",
    "    soup = BeautifulSoup(proxydb_html_page, 'html.parser')\n",
    "    tags = soup.find('dl', class_='row').find_all('dd')\n",
    "    ip = tags[0].text\n",
    "    anonymity_level = tags[1].text\n",
    "    country = tags[2].find('img')['alt']\n",
    "    city = tags[3].text\n",
    "    region = tags[4].text\n",
    "    isp = tags[5].text\n",
    "    printmd(\"**Proxy Anonymity Test Results:**\")\n",
    "    print(\"IP: \" + ip)\n",
    "    print(\"ANONYMITY-LEVEL: \" + anonymity_level)\n",
    "    print(\"COUNTRY: \" + country)\n",
    "    print(\"CITY: \" + city)\n",
    "    print(\"REGION: \" + region)\n",
    "    print(\"ISP: \" + isp)\n",
    "    printmd(\"**HTTP-Request Headers:**\")\n",
    "    tag = soup.find('pre')\n",
    "    print(tag.text)\n",
    "    \n",
    "#takes a selenium webdriver object and the xpath of the saved search we want to click on (defined above) and\n",
    "#logs in centris, goes to centris and clicks on the correspodning saved search\n",
    "# **NOTE: You have 20 seconds to enter username and password for athentication in google chrome\n",
    "def login_to_centris_and_go_to_a_saved_searchs_gallery(driver, xpath_of_saved_search):\n",
    "    \n",
    "    driver.get(\"https://www.centris.ca/en/login\")\n",
    "    #gonna be prompted to login proxy here\n",
    "    time.sleep(randint(20,24))\n",
    "    \n",
    "    #login\n",
    "    usernameBox=driver.find_element_by_id(\"loginradius-login-emailid\")\n",
    "    usernameBox.send_keys(\"davo98@hotmail.ca\")\n",
    "    passwordBox=driver.find_element_by_id(\"loginradius-login-password\")\n",
    "    passwordBox.send_keys(\"1111QQqq\")\n",
    "    time.sleep(randint(5,7))\n",
    "    submitButton = driver.find_element_by_id(\"loginradius-submit-login\")\n",
    "    submitButton.click()\n",
    "    time.sleep(randint(5,7))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #click on myaccount and then click on mysearches in the dropdown\n",
    "    my_account_button = driver.find_element_by_xpath('//*[@id=\"userDropdownMenuLink\"]')\n",
    "    my_account_button.click()\n",
    "    time.sleep(randint(5,7))\n",
    "    my_searches_dropdown_item = driver.find_element_by_xpath('//*[@id=\"header\"]/nav/ul/li[7]/div/a[1]')\n",
    "    my_searches_dropdown_item.click()\n",
    "    time.sleep(randint(5,7))        \n",
    "    #click the wanted search by using it's xpath.\n",
    "    saved_search_view_button = driver.find_element_by_xpath(xpath_of_saved_search)\n",
    "    saved_search_view_button.click()\n",
    "    \n",
    "def click_next_page_on_centris_gallery(driver):\n",
    "    driver.find_element_by_xpath('//*[@id=\"divWrapperPager\"]/ul/li[4]/a').click()\n",
    "     \n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#Getter functions\n",
    "    #scrapes a single listing for corresponding information\n",
    "    #append info to corresponding list\n",
    "def get_url(listing):\n",
    "    url_start = 'https://www.centris.ca'\n",
    "    url_end = listing.find('div', class_='thumbnail property-thumbnail-feature').find('a')['href']\n",
    "    url = url_start + url_end\n",
    "    urls.append(url)\n",
    "    return url\n",
    "def get_price(listing):\n",
    "    tags = listing.find('div', class_='price').find_all('span')\n",
    "    #the price might be in square meters or feet, so also look at the description below it to know the units\n",
    "    if 'square foot' in tags[1].text:\n",
    "        price_per_square_foot = float(tags[0]['content'])\n",
    "        price_per_square_meter = None\n",
    "        price = None\n",
    "    elif (('square metre' in tags[1].text) or ('square meter' in tags[1].text)):\n",
    "        price_per_square_foot = None\n",
    "        price_per_square_meter = float(tags[0]['content'])\n",
    "        price = None\n",
    "    \n",
    "    else:\n",
    "        price_per_square_foot = None\n",
    "        price_per_square_meter = None\n",
    "        price = int(tags[0]['content'])\n",
    "                    \n",
    "    prices.append(price)\n",
    "    ppsf.append(price_per_square_foot)\n",
    "    ppsm.append(price_per_square_meter)\n",
    "    return (price, price_per_square_foot, price_per_square_meter)               \n",
    "def get_category(listing):\n",
    "    category_text = listing.find('span', class_='category').find('div').text\n",
    "    category = category_text.split('\\xa0')[0][25:]\n",
    "    categories.append(category)\n",
    "    return category\n",
    "def get_adress_and_city(listing):\n",
    "    divs = listing.find('span', class_='address').find_all('div')\n",
    "    street = divs[0].text\n",
    "    city = divs[1].text\n",
    "    streets.append(street); cities.append(city)\n",
    "    return street, city       \n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "#scrapes all the listings on gallery page by calling the getters and prints the data\n",
    "#returns False if there are no listings on the page\n",
    "def scrape_a_gallery_page(page_html, page_number):\n",
    "    listings = page_html.find_all('div', class_='shell')\n",
    "    print('Number of listings on page: {}'.format(len(listings)))\n",
    "    \n",
    "    #check if there are listings on this page\n",
    "    if len(listings) == 0:\n",
    "        print(\"We're done\")\n",
    "        return False\n",
    "\n",
    "    #go through the listings on the page and run the functions defined above to fill the lists\n",
    "    for listing_number, listing in enumerate(listings, start=1): \n",
    " \n",
    "        print(f\"\\tpage: {page_number}, listing: {listing_number}\")\n",
    "    \n",
    "        #url \n",
    "        url = get_url(listing)\n",
    "        print(f\"\\t\\tURL: {url}\")\n",
    "        \n",
    "        #prices\n",
    "        price, price_per_square_foot, price_per_square_meter = get_price(listing)\n",
    "        print(f'\\t\\tPrice: {price}\\n\\t\\tper square foot: {price_per_square_foot}\\n\\t\\tper square meter: {price_per_square_meter}')\n",
    "        \n",
    "        #categories        \n",
    "        print(\"\\t\\tCategory: {}\".format(get_category(listing)))\n",
    "        \n",
    "        #address and city\n",
    "        street, city = get_adress_and_city(listing)\n",
    "        print(f\"\\t\\tStreet: {street}\\n\\t\\tCity: {city}\")\n",
    "        print(\"\\n\")\n",
    "    return True\n",
    "\n",
    "#takes in a driver that is on the first page of a gallery\n",
    "#and calls scrape_a_gallery_page on each page of the gallery\n",
    "def scrape_gallery(driver, number_of_pages_in_gallery):\n",
    "    for page_number in range(1, number_of_pages_in_gallery+1):\n",
    "        gallery_page_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        #scrape a gallery will return false if there are no listings on the page\n",
    "        if scrape_a_gallery_page(gallery_page_soup, page_number) == False:\n",
    "            break\n",
    "        else:\n",
    "            click_next_page_on_centris_gallery(driver)\n",
    "            time.sleep(randint(7,10))\n",
    "            \n",
    "#creates a driver, calls login_to_centris... then\n",
    "#gets number of pages in gallery and calls scrape_gallery \n",
    "def scrape_centris(xpath_of_saved_search, path_to_chrome_driver):\n",
    "    driver=webdriver.Chrome(executable_path=path_to_chrome_driver)\n",
    "    login_to_centris_and_go_to_a_saved_searchs_gallery(driver, xpath_of_saved_search)\n",
    "    time.sleep(randint(4,6))\n",
    "    number_of_pages_in_gallery = int(BeautifulSoup(driver.page_source, 'html.parser').find('li', class_='pager-current').text.split('/')[1])\n",
    "    print(\"Number of pages in gallery: {}\".format(number_of_pages_in_gallery))\n",
    "    scrape_gallery(driver, number_of_pages_in_gallery)  \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#these are the xpaths of the \"view\" buttons of the searches on the my-search page\n",
    "xpath_lands_search = '//*[@id=\"MySearches\"]/div/div/div/div[1]/div[2]/div/a[2]'\n",
    "xpath_industrial_search = '//*[@id=\"MySearches\"]/div/div/div/div[2]/div[2]/div/a[2]'\n",
    "\n",
    "#path to the chrome driver corresponding to version of chrome on laptop\n",
    "path_to_chrome_driver = r\"C:\\Users\\davo9\\anaconda3\\envs\\webscraping\\chromedriver_win32\\chromedriver_87.exe\"\n",
    "\n",
    "#enter the desired xpath\n",
    "scrape_centris(xpath_lands_search, path_to_chrome_driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Partition the dataframe for the proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#make a dataframe using the lists \n",
    "def make_dataframe():\n",
    "    for i in (len(prices), len(categories), len(streets), len(cities),  len(urls), len(ppsf), len(ppsm)):\n",
    "        print(i)\n",
    "    d = {'Category': categories, 'Price': prices, 'PPSF': ppsf, 'PPSM':ppsm, 'Street': streets, 'City': cities, 'URL': urls}\n",
    "    return pd.DataFrame(d)\n",
    "df = make_dataframe()\n",
    "df\n",
    "%store df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#partition df and %store (you need to specify how many proxies you want)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "#get the current proxies \n",
    "#note that when we read the csv, we actually read a dataframe, so we gotta convert to list\n",
    "#also, since to_csv saves dictionaries as strings, when we read it we gotta map(ast.literal_eval) to convert strings back to dict\n",
    "proxies = pd.read_csv(r'C:\\Users\\davo9\\anaconda3\\envs\\webscraping\\Proxies\\Current_Proxies.csv', index_col=0).iloc[:, 0].map(ast.literal_eval).tolist()\n",
    "#take a slice if you don't wanna use all the proxies\n",
    "proxies = proxies[9:12]\n",
    "\n",
    "#Adds new columns to be filled to the dataframe and\n",
    "#Partitions the df in terms of rows by dividing it into n dataframes where n is the number of proxies\n",
    "#Returns a list of the n dataframes\n",
    "#Note that the n dataframes are just copies of slices of the df, changes to these dataframes won't affect df\n",
    "def partition_dataframe(df, proxies):\n",
    "    #add columns\n",
    "    columns = ['Rooms','Bedrooms','Bathrooms','Lot area', 'Year built', 'Additional features', 'Potential gross revenue', 'Zoning', 'Description', 'Use of property', 'Available area', 'Building area (at ground level)']\n",
    "    for column in columns:\n",
    "        df[column] = np.nan\n",
    "    \n",
    "    #partition df by dividing it into n slices (n=number of proxies given)\n",
    "    number_of_proxies = len(proxies)\n",
    "    df_length = len(df.index)\n",
    "    interval = int(df_length/number_of_proxies)\n",
    "    df_partition = []\n",
    "    for i in range(0, number_of_proxies):\n",
    "        if i == number_of_proxies-1:\n",
    "            df_partition.append(df.loc[interval*i:df_length-1].copy())\n",
    "        else:\n",
    "            df_partition.append(df.loc[(interval*i):(interval*(i+1)-1)].copy())\n",
    "    print('''Number of rows in dataframe: {:>7}\\nNumber of proxies: {:>17}\\nLenght of intervals: {:>15}\\nLength of last interval: {:>11}'''\n",
    "          .format(df_length, number_of_proxies, len(df_partition[0].index), len(df_partition[number_of_proxies-1].index)))\n",
    "    return df_partition \n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Create a partition of the dataframe \n",
    "partition_of_incomplete_df = partition_dataframe(df, proxies)\n",
    "\n",
    "#Store the parition and the proxies so the other notebooks can access them\n",
    "%store partition_of_incomplete_df proxies\n",
    "\n",
    "#now go run the corresponding number of proxy notebooks and come back here to clean and save "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreive, clean and save the completed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once we ran the proxy scripts, we retreive all the partition elements\n",
    "%store -r df0 df1 df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#manually concatenate the dataframe slices\n",
    "listings = pd.concat([df0, df1, df2])\n",
    "listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     24,
     40,
     56
    ]
   },
   "outputs": [],
   "source": [
    "#clean the data\n",
    "import ast\n",
    "\n",
    "#listings_type: one of ['land', 'industrial']. The cleaning slightly differs based on the listing types\n",
    "def clean_listings_data(listings, listings_type):\n",
    "    \n",
    "    #delete the columns\n",
    "#     if listings_type == 'land':\n",
    "#         listings.drop(['Rooms', 'Bedrooms', 'Bathrooms', 'Potential gross revenue'], axis=1, inplace=True)\n",
    "\n",
    "    #add column to indicate if we calculated the ppsf from the price\n",
    "    listings['PPSF calculated from price'] = False\n",
    "    listings['Google maps'] = None\n",
    "    listings['km'] = None\n",
    "    listings['Time'] = None\n",
    "\n",
    "    #check if any listings are listed with a price per square METER,\n",
    "    #if so we stop the program and manually deal with them. set those values to null once dealt with them in order to pass the if statement on the next run\n",
    "    if len(listings.loc[listings['PPSM'].isnull()==False].index) != 0:\n",
    "        raise ValueError('The following listings have a PPSM, please manually set their ppsf (or take care of them in the appropriate way) and set their ppsm value to None: {}'.format(listings.loc[listings['PPSM'].isnull()==False].index.values.tolist()))\n",
    "    #once there are no listings with ppsm , delete the PPSM column\n",
    "    listings.drop(['PPSM'], axis=1, inplace=True)\n",
    "    \n",
    "    #if an available area is of the form: From x sqft to Y sqft, convert to Y sqft\n",
    "    def extract_available_area(available_area_value):\n",
    "        if pd.isna(available_area_value):\n",
    "            return None\n",
    "        else: #if not None then it must be a string\n",
    "            words = available_area_value.split()\n",
    "            if words[0] == 'From':\n",
    "                return words[4] + ' ' + words[5]\n",
    "            else:\n",
    "                return available_area_value\n",
    "    listings['Available area'] = listings['Available area'].apply(extract_available_area)\n",
    "    \n",
    "    \n",
    "    #this function checks if all values in a column are either in sqft or null, \n",
    "    #if not, it raises exception and print the line number\n",
    "    #if yes, then it outputs the listings with that column converted to numeric (we use this function in clean_listings_data)\n",
    "    #returns the listings with the converted column\n",
    "    def check_if_in_sqft_or_null_and_convert_to_numeric(listings, column_name):\n",
    "        idx_of_null_or_in_sqft_areas = listings.index[ (listings[column_name].isnull()) | (listings[column_name].astype(str).apply(lambda x: x.split()[-1]=='sqft')) ]\n",
    "        \n",
    "        if len(idx_of_null_or_in_sqft_areas) != listings.shape[0]:\n",
    "            raise ValueError(f'The following listings have {column_name} not in sqft, please manually take care of them: {[x for x in range(listings.shape[0]) if x not in idx_of_null_or_in_sqft_areas]}')\n",
    "        #convert the lot area column to numeric type\n",
    "        listings[column_name] = pd.to_numeric(listings[column_name].astype(str).apply(lambda x: x.split()[0].replace(',', '')), errors='coerce')\n",
    "        return listings\n",
    "    #for [Lot area, Available area, Building area (at ground level)], \n",
    "    #we check if all values in the column are either null or in sqft. If not raise exception, if so convert to numeric\n",
    "    listings = check_if_in_sqft_or_null_and_convert_to_numeric(listings, 'Lot area')\n",
    "    listings = check_if_in_sqft_or_null_and_convert_to_numeric(listings, 'Available area')\n",
    "    listings = check_if_in_sqft_or_null_and_convert_to_numeric(listings, 'Building area (at ground level)')\n",
    "      \n",
    "    #now that all the areas are numeric, we can complete the missing prices and ppsf's\n",
    "    #returns the completed listings\n",
    "    def complete_ppsf_and_price(listings, listings_type):\n",
    "        if listings_type == 'land':\n",
    "            area_of_interest = 'Lot area'\n",
    "        elif listings_type == 'industrial':\n",
    "            area_of_interest = 'Available area'\n",
    "            \n",
    "        #get the indices  where 'PPSF' is null and\n",
    "        #at those indices, calculate ppsf by:\n",
    "            #PRICE/LOT AREA for 'land' listings and\n",
    "            #PRICE/Available area for 'industrial' listings \n",
    "        #set the 'ppsf calculated from price' slot to true\n",
    "        null_ppsf_boolean_array = listings['PPSF'].isnull()\n",
    "        if null_ppsf_boolean_array.any():\n",
    "            listings.loc[null_ppsf_boolean_array, 'PPSF'] = listings.loc[null_ppsf_boolean_array, 'Price'] / listings.loc[null_ppsf_boolean_array, area_of_interest]\n",
    "            listings.loc[null_ppsf_boolean_array, 'PPSF calculated from price'] = True\n",
    "            print(f'The listings: {null_ppsf_boolean_array.index[null_ppsf_boolean_array].tolist()} had null PPSF. Their PPSF have now been calculated with \"Price/{area_of_interest}\"')\n",
    "\n",
    "        #similarly calculate the missing prices by PPSF*area_of_interest\n",
    "        #no need to keep an indicator for this, cz if 'ppsf calculated from price' is false \n",
    "        #then this ('price calculated from ppsf') is true.\n",
    "        null_price_boolean_array = listings['Price'].isnull()\n",
    "        if null_price_boolean_array.any():\n",
    "            listings.loc[null_price_boolean_array, 'Price'] = listings.loc[null_price_boolean_array, 'PPSF'] * listings.loc[null_price_boolean_array, area_of_interest]\n",
    "            print(f'The listings: {null_price_boolean_array.index[null_price_boolean_array].tolist()} had null prices. Their prices have now been calculated with \"PPSF*{area_of_interest}\"')\n",
    "        \n",
    "        return listings\n",
    "        \n",
    "    listings = complete_ppsf_and_price(listings, listings_type)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return listings\n",
    "\n",
    "listings = clean_listings_data(listings, 'land')\n",
    "\n",
    "listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#calculate the duration and distance of each listing using google api\n",
    "def compute_dur_and_dist_using_google_api(row):\n",
    "    url ='https://maps.googleapis.com/maps/api/distancematrix/json?'\n",
    "    google_api_key = 'AIzaSyCwRGlfkSeMyWieXt402ccN5fM1tUS4XtU'\n",
    "    origin = '3785+John-Lyman+QC+Canada'\n",
    "    \n",
    "    dest = row['Street'] + '+' + row['City'] + '+QC' \n",
    "    dest = '+'.join(dest.replace(',', '').split())\n",
    "    \n",
    "    r = requests.get(url + '&origins=' + origin + '&destinations=' + dest + '&key=' + google_api_key)\n",
    "    print(r.json()); print('\\n')\n",
    "    distance = r.json()['rows'][0]['elements'][0]['distance']['value']\n",
    "    duration = r.json()['rows'][0]['elements'][0]['duration']['text']\n",
    "    \n",
    "    row['Time'] = duration\n",
    "    row['km'] = round(distance/1000)\n",
    "    return row\n",
    "\n",
    "listings = listings.apply(compute_dur_and_dist_using_google_api, axis=1) \n",
    "\n",
    "#sort the listings by ascending PPSF\n",
    "listings = listings.sort_values(by='PPSF', ignore_index=True)\n",
    "\n",
    "#Save statistics in the stats dataframe in order to write it to excel\n",
    "stats = listings.describe()\n",
    "listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the new listings and the ones that were delisted by comparing with the last listings\n",
    "#l = ['PPSF', 'Lot area', 'Price', 'Time', 'City', 'Zoning', 'URL', 'Google maps', 'Description', 'Category', 'Street' ,'Additional', 'features', 'Year built', 'km', 'PPSF calculated from price', 'PPSF calculated from PPSM']\n",
    "\n",
    "old = pd.read_excel(r'C:\\Users\\davo9\\anaconda3\\envs\\webscraping\\Webscraping_Projects\\Real_Estate\\Centris\\Data\\Centris_Waterfront_Lands_1.3Msqft+_2020-12-24.xlsx')\n",
    "old.drop(columns=['PPSF calculated from PPSM'], inplace=True)\n",
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "concat = pd.concat([listings, old], ignore_index=True)\n",
    "print([x for x in concat.loc[concat.duplicated(subset=['Street', 'City'], keep=False)].index if x not in concat.loc[concat.duplicated(subset=['Lot area', 'Street', 'City'], keep=False)].index ])\n",
    "concat.loc[156, '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in concat.loc[concat.duplicated(subset=['Street', 'City'], keep=False)].index:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reindexing for lands\n",
    "#listings = listings.reindex(columns = ['PPSF', 'Lot area', 'Price', 'Time', 'City', 'Zoning', 'URL', 'Google maps', 'Description', 'Category', 'Street', 'Additional features', 'Year built', 'km', 'PPSF calculated from price', 'PPSF calculated from PPSM'], copy=False)\n",
    "\n",
    "#reindexing for industrial\n",
    "#listings = listings.reindex(columns = ['PPSF', 'Available area', 'Lot area', 'Price', 'Use of property', 'City', 'Zoning', 'URL', 'Building area (at ground level)', 'Google maps', 'Description', 'Category', 'Street', 'Additional features', 'Time', 'Year built', 'km', 'PPSF calculated from price'], copy=False)\n",
    "\n",
    "listings.loc[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "from datetime import datetime\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "#saves the df and df.describe() into an excel file with two worksheets\n",
    "def save_xlsx(worksheet_dict, path):\n",
    "    writer = ExcelWriter(path)\n",
    "    for key in worksheet_dict:\n",
    "        #don't write the index for the listings but write it for the stats\n",
    "        if key == 'listings':\n",
    "            worksheet_dict[key].to_excel(writer, key, index=False)\n",
    "        else:\n",
    "            worksheet_dict[key].to_excel(writer, key, index=True)\n",
    "    writer.save()\n",
    "\n",
    "worksheet_dict = {'listings': listings, 'stats': stats}\n",
    "\n",
    "#for lands\n",
    "#save_xlsx(worksheet_dict = worksheet_dict, path=r'C:\\Users\\davo9\\anaconda3\\envs\\webscraping\\Webscraping_Projects\\Real_Estate\\Centris\\Data\\Centris_Waterfront_Lands_1.3Msqft+_{}.xlsx'.format(datetime.today().strftime('%Y-%m-%d')))\n",
    "#for industrial\n",
    "save_xlsx(worksheet_dict = worksheet_dict, path=r'C:\\Users\\davo9\\anaconda3\\envs\\webscraping\\Webscraping_Projects\\Real_Estate\\Centris\\Data\\Centris_industrial_listings_less-than_4M$_{}.xlsx'.format(datetime.today().strftime('%Y-%m-%d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:webscraping]",
   "language": "python",
   "name": "conda-env-webscraping-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
